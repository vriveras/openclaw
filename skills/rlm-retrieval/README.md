# RLM Retrieval for OpenClaw

![Test Results](https://img.shields.io/badge/hybrid_accuracy-99.8%25-brightgreen)
![RLM](https://img.shields.io/badge/RLM-99.7%25-brightgreen)
![Semantic](https://img.shields.io/badge/semantic-85%25-yellow)
![Test Cases](https://img.shields.io/badge/test_cases-4000-blue)

**Hybrid memory system with RLM retrieval (rlm-get, rlm-state, rlm-check). Enhanced matching: substring, compound, fuzzy, and concept expansion.**

Never lose conversation context between sessions. Combines multiple approaches:
1. **Enhanced RLM retrieval** â€” substring, compound splitting, fuzzy (Levenshtein), concepts
2. **Semantic search** (OpenClaw's built-in `memory_search`) â€” paraphrases, embeddings
3. **Temporal filtering** â€” "yesterday", "last week" â†’ search only relevant sessions

## Commands

| Command | Purpose |
|---------|---------|
| `rlm-get` | **Mandatory** state retrieval before answering history questions |
| `rlm-state` | Show context state, active topics, threads |
| `rlm-check` | Validate retrieval accuracy |

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       HYBRID RETRIEVAL                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Query arrives                                                  â”‚
â”‚       â†“                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ ğŸ”® Semantic Search  â”‚    â”‚ ğŸ” RLM Retrieval    â”‚            â”‚
â”‚  â”‚ (memory_search)     â”‚    â”‚ (rlm-get)           â”‚            â”‚
â”‚  â”‚                     â”‚    â”‚                     â”‚            â”‚
â”‚  â”‚ â€¢ Paraphrases       â”‚    â”‚ â€¢ Exact matches     â”‚            â”‚
â”‚  â”‚ â€¢ Conceptual sim    â”‚    â”‚ â€¢ Code blocks       â”‚            â”‚
â”‚  â”‚ â€¢ Fuzzy intent      â”‚    â”‚ â€¢ URLs, names       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                        â†“                                        â”‚
â”‚              Merge & dedupe results                             â”‚
â”‚              Return with source indicator                       â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ~/.clawdbot/agents/<agentId>/sessions/                         â”‚
â”‚  â”œâ”€â”€ abc123.jsonl    â† Every message, tool call, timestamp      â”‚
â”‚  â”œâ”€â”€ def456.jsonl                                               â”‚
â”‚  â””â”€â”€ ...                                                        â”‚
â”‚                                                                 â”‚
â”‚  memory/                                                        â”‚
â”‚  â””â”€â”€ context-state.json  â† Topics, threads, decisions           â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

### One-liner (recommended)

```bash
curl -fsSL https://raw.githubusercontent.com/vriveras/openclaw-rlm-retrieval/master/install.sh | bash
```

### Manual install

```bash
cd ~/clawd/skills  # or your OpenClaw workspace
git clone https://github.com/vriveras/openclaw-rlm-retrieval.git rlm-retrieval
```

Restart OpenClaw after install.

## Getting Started

### 1. Build the session index

```bash
python skills/rlm-retrieval/scripts/index-sessions.py --agent-id main
```

### 2. Add to AGENTS.md (makes it mandatory on every session)

Add this to your workspace `AGENTS.md`:

```markdown
### ğŸ” RLM Retrieval Skill (MANDATORY)
When answering questions about past conversations, decisions, *status*, or history:

**YOU MUST CALL `rlm-get` BEFORE ANSWERING.**

Hard trigger phrases (non-optional):
- "where are we withâ€¦"
- "did you alreadyâ€¦"
- "what happened toâ€¦"
- "status ofâ€¦"

Process:
1. **Invoke RLM retrieval (`rlm-get`)** â€” canonical state lookup
2. **Run temporal search** â€” `python skills/rlm-retrieval/scripts/temporal_search.py "query"`
3. **Show retrieval indicators** â€” ğŸ”® semantic, ğŸ” RLM, ğŸ§  both
4. **Keep index fresh** â€” auto-refresh if stale (>2h), otherwise run manually

This ensures you can recall past context even after compaction. The session index + temporal parser narrow searches from 100+ sessions to just the relevant few.
```

### 3. Add to HEARTBEAT.md (keeps everything fresh)

Add this to your workspace `HEARTBEAT.md`:

```markdown
### Session Index Refresh (every few hours)
Check if session index is stale and refresh:
\`\`\`bash
# Only if >2 hours since last update
python skills/rlm-retrieval/scripts/index-sessions.py --agent-id main
\`\`\`
Last indexed: check `skills/rlm-retrieval/memory/sessions-index.json` â†’ `lastUpdated`

### Context State Update (after significant work)
If meaningful work was done since last update, refresh `memory/context-state.json`:
- Add new decisions made
- Update active topics
- Track new entities/projects  
- Mark threads as done/active

Check `lastUpdated` field - if >4 hours stale AND work was done, update it.

This is CRITICAL - the state file powers:
- Topic matching in filter_by_priors (boosts relevance scores)
- Decision queries ("what did we decide about X")
- Entity/concept expansion
```

### 4. Test it

```bash
# Search with temporal awareness (rlm-get)
python skills/rlm-retrieval/scripts/temporal_search.py "what did we discuss yesterday?"

# Should show:
# ğŸ“… Temporal filter: yesterday â†’ 2026-01-29
#    Filtered to 35 sessions (was 106)
```

Now the skill will be used on every session, even after compaction!

## Retrieval Indicators

When memory retrieval helps answer a question, you'll see which method found it:

| Indicator | Meaning |
|-----------|---------|
| ğŸ”® | Found via **semantic search** (embeddings) |
| ğŸ” | Found via **RLM keyword search** (direct grep) |
| ğŸ§  | Found via **both methods** (highest confidence) |

**Examples:**
```
ğŸ”® (semantic) We discussed authentication patterns yesterday.

ğŸ” (RLM) The exact error was: `ECONNREFUSED 127.0.0.1:5432`

ğŸ§  (both) We decided to use tmux because it handles WSL well.
```

## Commands

| Command | Action |
|---------|--------|
| `context help` | Explain available commands |
| `context state` | Show stats + active topics/threads/decisions |
| `context save` | Update state.json with current context |
| `context search X` | RLM search across transcripts |
| `remember this: ...` | Add to state |
| `what did we decide about X` | Search decisions (shows ğŸ§ ) |
| `where were we` | Show active threads |

## Context State

Ask `context state` to see your memory status:

```
ğŸ§  Context Memory Status
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š System Stats
   â€¢ Session transcripts: 47
   â€¢ Total transcript size: 12.3 MB
   â€¢ State entries: 12 (4 topics, 3 threads, 5 decisions)
   â€¢ Oldest session: 2026-01-15
   â€¢ Latest session: 2026-01-30

ğŸ“ Active Topics: context-memory, terminal-relay, wlxc

ğŸ§µ Open Threads
   â€¢ rlm-refactor (active) â€” Using raw transcripts instead of chunks
   â€¢ auth-flow (done)

ğŸ“‹ Recent Decisions
   â€¢ 01-30: Use raw JSONL transcripts, not curated chunks
   â€¢ 01-29: Use ğŸ§  emoji as RLM indicator
```

## Scripts

### Update Context State
Maintain the context-state.json file (critical for skill to work):
```bash
# Show current state
python scripts/update-state.py --show

# Add a topic
python scripts/update-state.py --topic "auth-refactor"

# Add a decision
python scripts/update-state.py --decision "Use OAuth2 PKCE" --context "Better security for SPAs"

# Add an entity
python scripts/update-state.py --entity "wlxc=Windows/Linux container runtime"

# Add/update a thread
python scripts/update-state.py --thread "auth-flow:active:Implementing OAuth2 PKCE"
```

### Temporal Search
Search with time awareness:
```bash
python scripts/temporal_search.py "what did we discuss yesterday?"
# â†’ Filters to yesterday's sessions before searching

python scripts/temporal_search.py "auth decisions last week"
# â†’ Filters to last week's sessions
```

### Session Index
Build/refresh the session index:
```bash
python scripts/index-sessions.py --agent-id main
```

### Search transcripts for a keyword
```bash
./scripts/search-transcripts.sh "authentication" main
```

### Direct JSONL search
```bash
rg -l "keyword" ~/.clawdbot/agents/main/sessions/*.jsonl
```

### Extract text from a session
```bash
jq -r 'select(.type=="message") | .message.content[]? | 
       select(.type=="text") | .text' <session>.jsonl
```

## State Schema

```json
{
  "lastUpdated": "2026-01-30T06:35:00Z",
  "activeTopics": ["auth", "api-design"],
  "openThreads": [
    {
      "id": "oauth-flow",
      "status": "active",
      "summary": "Implementing OAuth2 PKCE"
    }
  ],
  "recentDecisions": [
    {
      "date": "2026-01-30",
      "decision": "Use raw transcripts, not summaries"
    }
  ],
  "entities": {
    "wlxc": "Windows/Linux container runtime"
  }
}
```

## RLM Search Heuristics

From [arxiv.org/abs/2512.24601](https://arxiv.org/abs/2512.24601):

1. **Score by recency**: Today +3, yesterday +2, this week +1
2. **Score by topic**: Match against active topics +2
3. **Search top candidates only**: Don't scan everything
4. **Constant-size output**: Return top 5 results, never blow up context

## Why Raw Transcripts > Summaries

### Compaction vs RLM Analysis

![Compaction vs RLM](tests/compaction-vs-rlm.png)

**What survives 20% compaction?**

| Info Type | Survival Rate |
|-----------|---------------|
| Code blocks | âŒ 7% |
| URLs | âŒ 22% |
| Dates | âŒ 24% |
| Paths | âŒ 28% |
| Decisions | âŒ 29% |
| Names | âŒ 33% |
| Technical terms | âœ… 93% |
| **Average** | **34%** |

**50k token budget comparison:**
- **RLM (search)**: 58,617 facts accessible (100%)
- **Compacted**: 515 facts accessible (1%)

### Token Usage vs Data Loss

![Token vs Data Tradeoff](tests/token-vs-data.png)

**The tradeoff:**
- ğŸŸ  **Compaction** saves storage tokens but loses **66% of data**
- ğŸŸ¢ **RLM** stores more but retrieves with **10x fewer tokens per query**

| What You Spend | Clean + RLM | Compacted |
|----------------|-------------|-----------|
| Per session | 117k tokens | 23k tokens |
| Per query | **5k tokens** | 50k tokens |
| 10 sessions | 1,170k tokens | 230k tokens |

| What You Lose | Compaction Loss |
|---------------|-----------------|
| Code blocks | âŒ **93% lost** |
| URLs | âŒ 78% lost |
| Decisions | âŒ 71% lost |
| Tech terms | âœ… 7% lost |

**Bottom line:** Compaction saves 80% storage but you query 10x more tokens AND lose most of your data. RLM keeps everything and retrieves efficiently.

### Context Window Efficiency

![Context Window Efficiency](tests/context-window-efficiency.png)

**Per token in your context window:**

| Metric | RLM | Compacted |
|--------|-----|-----------|
| Useful tokens | **95%** | 15% |
| Wasted tokens | 5% | 85% |
| Facts per 5k tokens | **1,000** | 20 |

**RLM retrieves 50x more useful information per context token.**

Why? Compaction stuffs your context with summarized everything, mostly irrelevant. RLM retrieves only what's relevant to your query.

### The Bottom Line

- **No information loss** â€” summaries lose 66% of detail
- **Automatic** â€” Clawdbot already saves transcripts
- **Flexible** â€” search for anything, not just what was "curated"
- **True to RLM** â€” the paper's approach

## Enhanced Matching

The skill uses four matching strategies that combine for 99.8% accuracy:

### 1. Substring Matching
Query word contained in content word:
```
"Glicko" matches "Glicko-2 rating system"
"App" matches "AppData folder"
```

### 2. Compound Splitting
Splits camelCase, kebab-case, snake_case:
```
"ReadMessageItem" â†’ read, message, item
"context-memory" â†’ context, memory
"HostWindowsContainer" â†’ host, windows, container
```

### 3. Fuzzy Matching (Levenshtein â‰¤ 2)
Catches typos and close variants:
```
"postgres" â‰ˆ "PostgreSQL"
"javascrpt" â‰ˆ "javascript"
```

### 4. Concept Index
Related terms expansion:
```
"glicko" â†’ rating, chess, elo, leaderboard
"oauth" â†’ auth, authentication, token, security
"wlxc" â†’ container, windows, linux, containerd
```

## Test Results

Retrieval accuracy validated against **2000 test cases** across 10 categories.

### Overall Accuracy

![Test Results](tests/results-2000.png)

| Mode | Accuracy | Tests Passed |
|------|----------|--------------|
| **Hybrid** | **99.8%** | 1995/2000 |
| RLM (enhanced) | 99.6% | 1992/2000 |
| Semantic | 82.9% | 1658/2000 |
| Baseline (no memory) | ~35% | â€” |

### Category Breakdown

![Heatmap](tests/results-heatmap.png)

| Category | Hybrid | RLM | Notes |
|----------|--------|-----|-------|
| adversarial | âœ… 100% | 100% | Correctly rejects false queries |
| identity | âœ… 100% | 100% | People/contacts |
| vague | âœ… 100% | 100% | Ambiguous queries |
| project | âœ… 100% | 100% | Project status |
| temporal | âœ… 100% | 100% | Date handling |
| technical | âœ… 100% | 100% | Implementation details |
| decision | âœ… 100% | 100% | Past decisions |
| variation | âœ… 100% | 100% | Phrasing differences |
| partial | âœ… 96% | 94% | Single keywords |
| metadata | âœ… 100% | 100% | Runtime state |

## Latency Benchmark

Tradeoff between speed and recall:

| Metric | Basic (Raw) | Enhanced (Hybrid) | Change |
|--------|-------------|-------------------|--------|
| **Mean Latency** | 0.02ms | 14.78ms | +14.76ms |
| **Median Latency** | 0.02ms | 15.03ms | |
| **P95 Latency** | 0.03ms | 22.29ms | |
| **Recall** | 74% | 100% | **+26%** |

**Tradeoff Analysis:**
- Latency cost: +14.76ms per query
- Recall gain: +26%
- Cost per 1% recall: 0.57ms

Run benchmarks:
```bash
python tests/benchmark_latency.py --iterations 100
```

### Running Validation Tests

```bash
cd skills/context-memory/tests

# Run specific mode
python3 run-baseline-2000.py --mode rlm
python3 run-baseline-2000.py --mode semantic
python3 run-baseline-2000.py --mode hybrid

# Compare all modes and generate charts
python3 run-baseline-2000.py --mode compare
python3 generate-comparison-chart.py
```

### Test Categories

| Category | # Tests | Example Query | 
|----------|---------|---------------|
| **adversarial** | 673 | "What database did we choose?" (never discussed) |
| **technical** | 333 | "Where does X store transcripts?" |
| **variation** | 219 | "contextmemory" vs "context-memory" |
| **project** | 218 | "What phases of X are complete?" |
| **decision** | 180 | "What approach did we take for X?" |
| **vague** | 170 | "How do we handle paths?" |
| **partial** | 133 | "Glicko" (partial of "Glicko-2") |
| **temporal** | 68 | "What did we work on yesterday?" |
| **identity** | 3 | "Who is X?" |
| **metadata** | 3 | "Tell me about skill X" |

## Cross-Platform Validation

The approach was validated on two platforms to prove it generalizes:

### Platform Comparison

![Cross-Platform Baseline](tests/cross-platform-baseline.png)

| Platform | Baseline (grep) | With Skill | Improvement |
|----------|-----------------|------------|-------------|
| **Clawdbot** | 35.4% | **99.8%** | +64.4% |
| **Claude Code** | 83.3% | **90.0%** | +6.7% |

### The Journey

![Journey Timeline](tests/journey-timeline.png)

From 35% baseline to 99.8% accuracy:
1. **No Memory** (35.4%) â€” Baseline without any memory system
2. **Basic RLM** (81%) â€” Simple keyword grep
3. **Semantic Search** (82.9%) â€” Embeddings alone
4. **Hybrid Basic** (89.8%) â€” Combined approach
5. **Enhanced Matching** (99.8%) â€” Substring + fuzzy + compound + concepts

### Category Improvements

![Category Improvement](tests/category-improvement-both.png)

**Clawdbot gains:**
- Partial matching: 35% â†’ 97% (+62%)
- Decision queries: 48% â†’ 100% (+52%)
- Temporal: 61% â†’ 100% (+39%)

**Claude Code gains:**
- Fuzzy (typos): 0% â†’ 75% (+75%)
- Compound terms: preserved at 100%
- Adversarial: 100% â†’ 88% (-12% from concept expansion)

### Why Different Baselines?

- **Clawdbot baseline (35.4%)**: No memory at all â€” pure LLM hallucination
- **Claude Code baseline (83.3%)**: Built-in grep/project search works for simple cases

The skill adds the most value where baseline fails: typos, partial words, compound terms, and related concepts.

## License

MIT
