# Evidence & Test Data Documentation

*Supporting documentation for all data claims in the RLM blog series*

## Part 1: "From 35% to 99.8%: Building a Memory System That Actually Works"

### Main Claims & Supporting Files

| Claim | Source File | Location | How to Reproduce |
|-------|-------------|----------|------------------|
| **35% baseline accuracy** (no memory) | `baseline-results-2000-rlm.json` | `tests/` | `python scripts/run_baseline.py --tests 2000` |
| **99.8% hybrid accuracy** | `test-cases-2000.json` + validation runs | `tests/` | `python scripts/run_2000_validation.py` |
| **4,000 tests, 99.8% pass** | `validation-4000-full.json` | `memory/` | `python scripts/run_4000_validation.py` (47 min) |
| **100-test sample validation** | `validation-100-sample.json` | `memory/` | `python scripts/validate_100_sample.py` |
| **Category breakdown table** | `test-cases-2000.json` | `tests/` | Analysis of test case distribution |
| **0.57ms per % improvement** | `benchmark-results.json` | `tests/` | `python scripts/benchmark_latency.py` |
| **86.5% live validation** | `real_test_cases.json` + `real_test_results.json` | `tests/` | `python scripts/run_real_tests.py` |

### Test File Details

#### Core Test Suites

**1. `tests/test-cases-2000.json`** (1.3MB)
- **Description:** Main 2,000 test case suite
- **Format:** JSON array of test objects with query, expected result, category
- **Categories:** adversarial (1,200), technical (600), project (400), variation (400), decision (320), vague (320), partial (240), identity (200), metadata (200), temporal (120)
- **Generated by:** `tests/generate-2000-tests.py`

**2. `memory/validation-4000-full.json`** (1.3KB summary)
- **Description:** Summary of 4,000 test validation run
- **Key metrics:** 4000/4000 passed, 715ms mean latency, 47.7 min total
- **Full results:** Stored in validation run output (not committed due to size)

**3. `tests/baseline-results-2000-rlm.json`** (845KB)
- **Description:** Baseline accuracy without any memory system
- **Result:** 35.4% accuracy (no memory), 82.9% (semantic only), 88.2% (RLM only), 89.8% (hybrid basic)

**4. `tests/real_test_cases.json`** (319KB)
- **Description:** 37 real-world queries from live sessions
- **Purpose:** Validation against actual conversation data, not synthetic tests
- **Result:** 86.5% accuracy in sub-agents, 93% in main session

**5. `tests/real_test_results.json`** (1.2KB)
- **Description:** Outcomes of real test cases
- **Shows:** Partial matches, temporal filtering, adversarial rejection rates

#### Validation Reports

**6. `memory/validation-100-sample.json`** (39KB)
- **Description:** 100-test quick validation
- **Purpose:** Fast verification of recall preservation
- **Result:** 100% recall, 1.0 top-3 match ratio

**7. `validation-results/claude-code-live-validation.json`**
- **Description:** Cross-platform validation on Claude Code
- **Result:** 100% on 21 tests (different platform, same code)

**8. `validation-results/validation-report.json`**
- **Description:** Aggregated validation metrics
- **Includes:** Per-category accuracy, latency breakdown, error analysis

### Compaction Analysis

**9. `tests/compaction-analysis.py`** (11KB)
- **Description:** Script that generates compaction survival data
- **Method:** Loads real transcripts, simulates compaction, measures fact survival
- **Run:** `python tests/compaction-analysis.py --all`
- **Latest results (255 sessions):**
  - Code blocks: 24% survive, 76% lost
  - URLs: 20% survive, 80% lost
  - Decisions: 13% survive, 87% lost
  - File paths: 21% survive, 79% lost
  - Names: 33% survive, 67% lost
  - Technical terms: 95% survive, 5% lost

### Generated Charts

All charts referenced in the blog are stored in:
- `tests/` — Source charts
- `validation-results/` — Analysis outputs

| Chart | Source File | Generated By |
|-------|-------------|----------------|
| Compaction vs RLM | `compaction-vs-rlm.png` | `tests/compaction-analysis.py` |
| Journey Timeline | `journey-timeline.png` | `tests/generate-baseline-charts.py` |
| Enhanced Matching Impact | `enhanced-matching-impact.png` | `tests/generate-before-after-chart.py` |
| Category Heatmap | `results-heatmap.png` | `tests/generate-results-chart.py` |
| Cross-Platform | `cross-platform-comparison.png` | `tests/generate-baseline-charts.py` |

### How to Reproduce Results

#### 1. Run 2,000 Test Validation (15-20 minutes)
```bash
cd ~/clawd/skills/rlm-retrieval
python3 scripts/run_2000_validation.py
```

#### 2. Run 100-Test Sample Validation (2 minutes)
```bash
cd ~/clawd/skills/rlm-retrieval
python3 scripts/validate_100_sample.py
```

#### 3. Run Real-World Test Validation (5 minutes)
```bash
cd ~/clawd/skills/rlm-retrieval
python3 scripts/run_real_tests.py
```

#### 4. Run Compaction Analysis (1 minute)
```bash
cd ~/clawd/skills/rlm-retrieval/tests
python3 compaction-analysis.py --all
```

#### 5. Regenerate All Charts
```bash
cd ~/clawd/skills/rlm-retrieval/tests
python3 generate-baseline-charts.py
python3 generate-before-after-chart.py
python3 generate-results-chart.py
python3 compaction-analysis.py --all
```

### Data Integrity Notes

- All test cases are **deterministic** — same input produces same output
- **No randomness** in the matching algorithms (substring, fuzzy, compound all deterministic)
- **Timestamped** — all validation runs include timestamps for reproducibility
- **Version controlled** — test case generators and validation scripts in git

### Test Generation Methodology

**Synthetic tests (2,000/4,000):**
- Generated from actual conversation transcripts
- Categories designed to stress-test specific failure modes
- Adversarial tests (30%) ensure correct rejection of false matches

**Real tests (37):**
- Actual queries from live sessions
- No cherry-picking — includes failures and edge cases
- Cross-platform validation (Clawdbot + Claude Code)

### Verification Checklist

Before publishing, verify:
- [ ] `tests/test-cases-2000.json` exists and is >1MB
- [ ] `memory/validation-4000-full.json` shows 4000/4000 passed
- [ ] `tests/compaction-analysis.py` produces survival rates matching blog
- [ ] All charts are committed to `gist-images` repo
- [ ] Git commits are pushed to both `my-writing` and `gist-images`

---

*Last updated: 2026-02-02*
*Validated on: 255 sessions, ~1M tokens, 4,000 test cases*
